{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Parameter tuning for range-separated models\n\n.. currentmodule:: torchpme\n\n:Authors: Michele Ceriotti [@ceriottm](https://github.com/ceriottm/)\n\nMetods to compute efficiently a long-range potential $v(r)$\nusually rely on partitioning it into a short-range part, evaluated\nas a sum over neighbor pairs, and a long-range part evaluated\nin reciprocal space\n\n\\begin{align}v(r)= v_{\\mathrm{SR}}(r) + v_{\\mathrm{LR}}(r)\\end{align}\n\nThe overall cost depend on the balance of multiple factors, that\nwe summarize here briefly to explain how the cost of evaluating\n$v(r)$ can be minimized, either manually or automatically.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import modules\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import ase\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport vesin.torch as vesin\n\nimport torchpme\nfrom torchpme.tuning.pme import PMEErrorBounds, tune_pme\nfrom torchpme.tuning.tuner import TuningTimings\n\ndevice = \"cpu\"\ndtype = torch.float64\nrng = torch.Generator()\nrng.manual_seed(42)\n\n# get_ipython().run_line_magic(\"matplotlib\", \"inline\")  # type: ignore # noqa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up a test system, a supercell containing atoms with a NaCl structure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "madelung_ref = 1.7475645946\nstructure = ase.Atoms(\n    positions=[\n        [0, 0, 0],\n        [1, 0, 0],\n        [0, 1, 0],\n        [1, 1, 0],\n        [0, 0, 1],\n        [1, 0, 1],\n        [0, 1, 1],\n        [1, 1, 1],\n    ],\n    cell=[2, 2, 2],\n    symbols=\"NaClClNaClNaNaCl\",\n)\nstructure = structure.repeat([2, 2, 2])\nnum_formula_units = len(structure) // 2\n\n# Uncomment these to add a displacement (energy won't match the Madelung constant)\n# displacement = torch.normal(\n#    mean=0.0, std=2.5e-1, size=(len(structure), 3), generator=rng\n# )\n# structure.positions += displacement.numpy()\n\npositions = torch.from_numpy(structure.positions).to(device=device, dtype=dtype)\ncell = torch.from_numpy(structure.cell.array).to(device=device, dtype=dtype)\n\ncharges = torch.tensor(\n    [[1.0], [-1.0], [-1.0], [1.0], [-1.0], [1.0], [1.0], [-1.0]]\n    * (len(structure) // 8),\n    dtype=dtype,\n    device=device,\n).reshape(-1, 1)\n\n# Uncomment these to randomize charges (energy won't match the Madelung constant)\n# charges += torch.normal(mean=0.0, std=1e-1, size=(len(charges), 1), generator=rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to evaluate the neighbor list; this is usually pre-computed\nby the code that calls `torch-pme`, and entails the first key parameter:\nthe cutoff used to compute the real-space potential $v_\\mathrm{SR}(r)$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_cutoff = 16.0\n\n# use `vesin`\nnl = vesin.NeighborList(cutoff=max_cutoff, full_list=False)\ni, j, S, d = nl.compute(points=positions, box=cell, periodic=True, quantities=\"ijSd\")\nneighbor_indices = torch.stack([i, j], dim=1)\nneighbor_shifts = S\nneighbor_distances = d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demonstrate errors and timings for PME\n\nTo set up a PME calculation, we need to define its basic parameters and\nsetup a few preliminary quantities.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The PME calculator has a few further parameters: ``smearing``, that determines\naggressive is the smoothing of the point charges. This makes the reciprocal-space\npart easier to compute, but makes $v_\\mathrm{SR}(r)$ decay more slowly,\nand error that we shall investigate further later on.\nThe mesh parameters involve both the spacing and the order of the interpolation\nused. Note that here we use :class:`CoulombPotential`, that computes a simple\n$1/r$ electrostatic interaction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smearing = 1.0\npme_params = {\"mesh_spacing\": 1.0, \"interpolation_nodes\": 4}\n\npme = torchpme.PMECalculator(\n    potential=torchpme.CoulombPotential(smearing=smearing),\n    **pme_params,  # type: ignore[arg-type]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the calculator\n\nWe combine the structure data and the neighbor list information to\ncompute the potential at the particle positions, and then the\nenergy\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "potential = pme(\n    charges=charges,\n    cell=cell,\n    positions=positions,\n    neighbor_indices=neighbor_indices,\n    neighbor_distances=neighbor_distances,\n)\n\nenergy = charges.T @ potential\nmadelung = (-energy / num_formula_units).flatten().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute error bounds (and timings)\n\nHere we calculate the potential energy of the system, and compare it with the\nmadelung constant to calculate the error. This is the actual error. Then we use\nthe :class:`torchpme.tuning.pme.PMEErrorBounds` to calculate the error bound for\nPME.\nError bounds are computed explicitly for a target structure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "error_bounds = PMEErrorBounds(charges, cell, positions)\n\nestimated_error = error_bounds(\n    cutoff=max_cutoff, smearing=smearing, **pme_params\n).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... and a similar class can be used to estimate the timings, that are assessed\nbased on a calculator (that should be initialized with the same parameters)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "timings = TuningTimings(\n    charges,\n    cell,\n    positions,\n    neighbor_indices=neighbor_indices,\n    neighbor_distances=neighbor_distances,\n    run_backward=True,\n)\nestimated_timing = timings(pme)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The error bound is estimated for the force acting on atoms, and is\nexpressed in force units - hence, the comparison with the Madelung constant\nerror can only be qualitative.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    f\"\"\"\nComputed madelung constant: {madelung}\nActual error: {madelung - madelung_ref}\nEstimated error: {estimated_error}\nTiming: {estimated_timing} seconds\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing the parameters of PME\n\nThere are many parameters that enter the implementation\nof a range-separated calculator like PME, and it is necessary\nto optimize them to obtain the best possible accuracy/cost tradeoff.\nIn most practical use cases, the cutoff is dictated by the external\ncalculator and is treated as a fixed parameter. In cases where\nperformance is critical, one may want to optimize this separately,\nwhich can be achieved easily with a grid or binary search.\n\nWe can set up easily a brute-force evaluation of the error as a\nfunction of these parameters, and use it to guide the design of\na more sophisticated optimization protocol.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def filter_neighbors(cutoff, neighbor_indices, neighbor_distances):\n    assert cutoff <= max_cutoff\n\n    filter_idx = torch.where(neighbor_distances <= cutoff)\n\n    return neighbor_indices[filter_idx], neighbor_distances[filter_idx]\n\n\ndef timed_madelung(cutoff, smearing, mesh_spacing, interpolation_nodes):\n    filter_indices, filter_distances = filter_neighbors(\n        cutoff, neighbor_indices, neighbor_distances\n    )\n\n    pme = torchpme.PMECalculator(\n        potential=torchpme.CoulombPotential(smearing=smearing),\n        mesh_spacing=mesh_spacing,\n        interpolation_nodes=interpolation_nodes,\n    )\n    potential = pme(\n        charges=charges,\n        cell=cell,\n        positions=positions,\n        neighbor_indices=filter_indices,\n        neighbor_distances=filter_distances,\n    )\n    energy = charges.T @ potential\n    madelung = (-energy / num_formula_units).flatten().item()\n\n    timings = TuningTimings(\n        charges,\n        cell,\n        positions,\n        neighbor_indices=filter_indices,\n        neighbor_distances=filter_distances,\n        run_backward=True,\n        n_warmup=1,\n        n_repeat=4,\n    )\n    estimated_timing = timings(pme)\n    return madelung, estimated_timing\n\n\nsmearing_grid = torch.logspace(-1, 0.5, 8)\nspacing_grid = torch.logspace(-1, 0.5, 9)\nresults = np.zeros((len(smearing_grid), len(spacing_grid)))\ntimings = np.zeros((len(smearing_grid), len(spacing_grid)))\nbounds = np.zeros((len(smearing_grid), len(spacing_grid)))\nfor ism, smearing in enumerate(smearing_grid):\n    for isp, spacing in enumerate(spacing_grid):\n        results[ism, isp], timings[ism, isp] = timed_madelung(8.0, smearing, spacing, 4)\n        bounds[ism, isp] = error_bounds(8.0, smearing, spacing, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now plot the error landscape. The estimated error can be seen as a upper bound of\nthe actual error. Though the magnitude of the estimated error is higher than the\nactual error, the trend is the same. Also, from the timing results, we can see that\nthe timing increases as the spacing decreases, while the smearing does not affect the\ntiming, because the interactions are computed up to the fixed cutoff regardless of\nwhether $v_\\mathrm{sr}(r)$ is negligible or large.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vmin = 1e-12\nvmax = 2\nlevels = np.geomspace(vmin, vmax, 30)\n\nfig, ax = plt.subplots(1, 3, figsize=(9, 3), sharey=True, constrained_layout=True)\ncontour = ax[0].contourf(\n    spacing_grid,\n    smearing_grid,\n    bounds,\n    vmin=vmin,\n    vmax=vmax,\n    levels=levels,\n    norm=mpl.colors.LogNorm(),\n    extend=\"both\",\n)\nax[0].set_xscale(\"log\")\nax[0].set_yscale(\"log\")\nax[0].set_ylabel(r\"$\\sigma$ / \u00c5\")\nax[0].set_xlabel(r\"spacing / \u00c5\")\nax[0].set_title(\"estimated error\")\ncbar = fig.colorbar(contour, ax=ax[1], label=\"error\")\ncbar.ax.set_yscale(\"log\")\n\ncontour = ax[1].contourf(\n    spacing_grid,\n    smearing_grid,\n    np.abs(results - madelung_ref),\n    vmin=vmin,\n    vmax=vmax,\n    levels=levels,\n    norm=mpl.colors.LogNorm(),\n    extend=\"both\",\n)\nax[1].set_xscale(\"log\")\nax[1].set_yscale(\"log\")\nax[1].set_xlabel(r\"spacing / \u00c5\")\nax[1].set_title(\"actual error\")\n\ncontour = ax[2].contourf(\n    spacing_grid,\n    smearing_grid,\n    timings,\n    levels=np.geomspace(1e-2, 5e-1, 20),\n    norm=mpl.colors.LogNorm(),\n)\nax[2].set_xscale(\"log\")\nax[2].set_yscale(\"log\")\nax[2].set_ylabel(r\"$\\sigma$ / \u00c5\")\nax[2].set_xlabel(r\"spacing / \u00c5\")\nax[2].set_title(\"actual timing\")\ncbar = fig.colorbar(contour, ax=ax[2], label=\"time / s\")\ncbar.ax.set_yscale(\"log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizing the smearing\nThe error is a sum of an error on the real-space evaluation of the\nshort-range potential, and of a long-range error. Considering the\ncutoff as given, the short-range error is determined easily by how\nquickly $v_\\mathrm{sr}(r)$ decays to zero, which depends on\nthe Gaussian smearing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smearing_grid = torch.logspace(-0.6, 1, 20)\nerr_vsr_grid = error_bounds.err_rspace(smearing_grid, torch.tensor([5.0]))\nerr_vlr_grid_4 = [\n    error_bounds.err_kspace(\n        torch.tensor([s]), torch.tensor([1.0]), torch.tensor([4], dtype=int)\n    )\n    for s in smearing_grid\n]\nerr_vlr_grid_2 = [\n    error_bounds.err_kspace(\n        torch.tensor([s]), torch.tensor([1.0]), torch.tensor([3], dtype=int)\n    )\n    for s in smearing_grid\n]\n\nfig, ax = plt.subplots(1, 1, figsize=(4, 3), constrained_layout=True)\nax.loglog(smearing_grid, err_vsr_grid, \"r-\", label=\"real-space\")\nax.loglog(smearing_grid, err_vlr_grid_4, \"b-\", label=\"k-space (spacing: 1\u00c5, n.int.: 4)\")\nax.loglog(smearing_grid, err_vlr_grid_2, \"c-\", label=\"k-space (spacing: 1\u00c5, n.int.: 2)\")\nax.set_ylabel(r\"estimated error / a.u.\")\nax.set_xlabel(r\"smearing / \u00c5\")\nax.set_title(\"cutoff = 5.0 \u00c5\")\nax.set_ylim(1e-20, 2)\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the simple, monotonic and fast-varying trend for the real-space error,\nit is easy to pick the optimal smearing as the value corresponding to roughly\nhalf of the target error -e.g. for a target accuracy of $1e^{-5}$,\none would pick a smearing of about 1\u00c5. Given that usually there is a\ncost/accuracy tradeoff, and smaller smearings make the reciprocal-space evaluation\nmore costly, the largest smearing is the best choice here.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizing mesh and interpolation order\nOnce the smearing value that gives an acceptable accuracy for the real-space\ncomponent has been determined, there may be other parameters that need to be\noptimized. One way to do this is to perform a grid search, and pick, among the\nparameters that yield an error below the threshold, those that empirically lead\nto the fastest evaluation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spacing_grid = torch.logspace(-1, 1, 10)\nnint_grid = [3, 4, 5, 6]\nresults = np.zeros((len(nint_grid), len(spacing_grid)))\ntimings = np.zeros((len(nint_grid), len(spacing_grid)))\nbounds = np.zeros((len(nint_grid), len(spacing_grid)))\nfor inint, nint in enumerate(nint_grid):\n    for isp, spacing in enumerate(spacing_grid):\n        results[inint, isp], timings[inint, isp] = timed_madelung(\n            5.0, 1.0, spacing, nint\n        )\n        bounds[inint, isp] = error_bounds(5.0, 1.0, spacing, nint)\n\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3), constrained_layout=True)\ncolors = [\"r\", \"#AA0066\", \"#6600AA\", \"b\"]\nlabels = [\n    \"smearing 1\u00c5, n.int: 3\",\n    \"smearing 1\u00c5, n.int: 4\",\n    \"smearing 1\u00c5, n.int: 5\",\n    \"smearing 1\u00c5, n.int: 6\",\n]\n\n# Plot original lines on ax[0]\nfor i in range(4):\n    ax[0].loglog(spacing_grid, bounds[i], \"-\", color=colors[i], label=labels[i])\n    ax[1].loglog(spacing_grid, timings[i], \"-\", color=colors[i], label=labels[i])\n    # Find where condition is met\n    condition = bounds[i] < 1e-5\n    # Overlay thicker markers at the points below threshold\n    ax[0].loglog(\n        spacing_grid[condition],\n        bounds[i][condition],\n        \"-o\",\n        linewidth=3,\n        markersize=4,\n        color=colors[i],\n    )\n    ax[1].loglog(\n        spacing_grid[condition],\n        timings[i][condition],\n        \"-o\",\n        linewidth=3,\n        markersize=4,\n        color=colors[i],\n    )\n\nax[0].set_ylabel(r\"estimated error / a.u.\")\nax[0].set_xlabel(r\"mesh spacing / \u00c5\")\nax[1].set_ylabel(r\"timing / s\")\nax[1].set_xlabel(r\"mesh spacing / \u00c5\")\nax[0].set_title(\"cutoff = 5.0 \u00c5\")\nax[0].set_ylim(1e-6, 2)\nax[0].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall errors saturate to the value of the real-space error,\nwhich is why we can pretty much fix the value of the smearing for a\ngiven cutoff. Higher interpolation orders allow to push the accuracy\nto higher values even with a large mesh spacing, resulting in large\ncomputational savings. However, depending on the specific setup,\nthe overhead associated with the more complex interpolation (that is\nseen in the coarse-mesh limit) could favor intermediate values\nof ``interpolation_order``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic tuning\nEven though these detailed examples are useful to understand the\nnumerics of PME, and the logic one could follow to pick the best\nvalues, in practice one may want to automate the procedure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smearing, parameters, timing = tune_pme(\n    accuracy=1e-5,\n    charges=charges,\n    cell=cell,\n    positions=positions,\n    cutoff=5.0,\n    neighbor_indices=neighbor_indices,\n    neighbor_distances=neighbor_distances,\n)\n\nprint(f\"\"\"\nEstimated PME parameters (cutoff={5.0} \u00c5):\nSmearing: {smearing} \u00c5\nMesh spacing: {parameters[\"mesh_spacing\"]} \u00c5\nInterpolation order: {parameters[\"interpolation_nodes\"]}\nEstimated time per step: {timing} s\n\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is the best cutoff?\nDetermining the most efficient cutoff value can be achieved by\nrunning a simple search over a few \"reasonable\" values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cutoff_grid = torch.tensor([3.0, 4.0, 5.0, 6.0, 7.0, 8.0])\n\ntimings_grid = []\nfor cutoff in cutoff_grid:\n    filter_indices, filter_distances = filter_neighbors(\n        cutoff, neighbor_indices, neighbor_distances\n    )\n    smearing, parameters, timing = tune_pme(\n        accuracy=1e-5,\n        charges=charges,\n        cell=cell,\n        positions=positions,\n        cutoff=cutoff,\n        neighbor_indices=filter_indices,\n        neighbor_distances=filter_distances,\n    )\n    timings_grid.append(timing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even though the trend is smooth, there is substantial variability,\nindicating it may be worth to perform this additional tuning whenever\nthe long-range model is the bottleneck of a calculation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(4, 3), constrained_layout=True)\nax.plot(cutoff_grid, timings_grid, \"r-*\")\nax.set_ylabel(r\"avg. timings / s\")\nax.set_xlabel(r\"cutoff / \u00c5\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}