{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Custom models with automatic differentiation\n\n.. currentmodule:: torchpme\n\n:Authors: Michele Ceriotti [@ceriottm](https://github.com/ceriottm/)\n\nThis example showcases how the main building blocks of ``torchpme``,\n:class:`MeshInterpolator` and :class:`KSpacaFilter` can be combined creatively to\nconstruct arbitrary models that incorporate long-range structural correlations.\n\nNone of the models presented here has probably much meaning, and the use in a ML setting\n(including the definition of an appropriate loss, and its optimization) is left as an\nexercise to the reader.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\n\nimport ase\nimport torch\n\nimport torchpme\n\ndevice = \"cpu\"\ndtype = torch.float64\nrng = torch.Generator()\nrng.manual_seed(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a trial structure -- a distorted rocksalt structure\nwith perturbed positions and charges\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "structure = ase.Atoms(\n    positions=[\n        [0, 0, 0],\n        [3, 0, 0],\n        [0, 3, 0],\n        [3, 3, 0],\n        [0, 0, 3],\n        [3, 0, 3],\n        [0, 3, 3],\n        [3, 3, 3],\n    ],\n    cell=[6, 6, 6],\n    symbols=\"NaClClNaClNaNaCl\",\n)\n\ndisplacement = torch.normal(\n    mean=0.0, std=2.5e-1, size=(len(structure), 3), generator=rng\n)\nstructure.positions += displacement.numpy()\n\ncharges = torch.tensor(\n    [[1.0], [-1.0], [-1.0], [1.0], [-1.0], [1.0], [1.0], [-1.0]],\n    dtype=dtype,\n    device=device,\n)\ncharges += torch.normal(mean=0.0, std=1e-1, size=(len(charges), 1), generator=rng)\npositions = torch.from_numpy(structure.positions).to(device=device, dtype=dtype)\ncell = torch.from_numpy(structure.cell.array).to(device=device, dtype=dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Autodifferentiation through the core ``torchpme`` classes\nWe begin by showing how it is possible to compute a function of the internal state\nfor the core classes, and to differentiate with respect to the structural and input\nparameters.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions of the atom density\n\nThe construction of a \"decorated atom density\" through\n:class:`MeshInterpolator <torchpme.lib.MeshInterpolator>`\ncan be easily differentiated through.\nWe only need to request a gradient evaluation, evaluate the grid, and compute\na function of the grid points (again, this is a proof-of-principle example,\nprobably not very useful in practice).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "positions.requires_grad_(True)\ncharges.requires_grad_(True)\ncell.requires_grad_(True)\n\nns = torch.tensor([5, 5, 5])\ninterpolator = torchpme.lib.MeshInterpolator(\n    cell=cell, ns_mesh=ns, interpolation_nodes=3, method=\"Lagrange\"\n)\ninterpolator.compute_weights(positions)\nmesh = interpolator.points_to_mesh(charges)\n\nvalue = mesh.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradients can be computed by just running `backward` on the\nend result.\nBecause of the sum rules that apply to the interpolation scheme,\nthe gradients with respect to positions and cell entries are zero,\nand the gradients relative to the charges are all 1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# we keep the graph to compute another quantity\nvalue.backward(retain_graph=True)\n\nprint(\n    f\"\"\"\nPosition gradients:\n{positions.grad.T}\n\nCell gradients:\n{cell.grad}\n\nCharges gradients:\n{charges.grad.T}\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we apply a non-linear function before summing,\nthese sum rules apply only approximately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "positions.grad.zero_()\ncharges.grad.zero_()\ncell.grad.zero_()\n\nvalue2 = torch.sin(mesh).sum()\nvalue2.backward(retain_graph=True)\n\nprint(\n    f\"\"\"\nPosition gradients:\n{positions.grad.T}\n\nCell gradients:\n{cell.grad}\n\nCharges gradients:\n{charges.grad.T}\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Indirect functions of the weights\n\nIt is possible to have the atomic weights be a\nfunction of other quantities. For instance, pretend\nthere is an external electric field along $x$,\nand that the weights should be proportional to the\nelectrostatic energy at each atom position\n(NB: defining an electric field in a periodic setting is\nnot so simple, this is just a toy example).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "positions.grad.zero_()\ncharges.grad.zero_()\ncell.grad.zero_()\n\nweights = charges * positions[:, :1]\nmesh3 = interpolator.points_to_mesh(weights)\n\nvalue3 = mesh3.sum()\nvalue3.backward()\n\nprint(\n    f\"\"\"\nPosition gradients:\n{positions.grad.T}\n\nCell gradients:\n{cell.grad}\n\nCharges gradients:\n{charges.grad.T}\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizable k-space filter\nThe operations in a\n:class:`KSpaceFilter <torchpme.lib.KSpaceFilter>`\ncan also be differentiated through.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A parametric k-space filter\nWe define a filter with multiple smearing parameters,\nthat are applied separately to multiple mesh channels\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ParametricKernel(torch.nn.Module):\n    def __init__(self, sigma: torch.Tensor, a0: torch.Tensor):\n        super().__init__()\n        self._sigma = sigma\n        self._a0 = a0\n\n    def kernel_from_k_sq(self, k_sq):\n        filter = torch.stack([torch.exp(-k_sq * s**2 / 2) for s in self._sigma])\n        filter[0, :] *= self._a0[0] / (1 + k_sq)\n        filter[1, :] *= self._a0[1] / (1 + k_sq**3)\n        return filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a 2D weights (to get a 2D mesh), and\ndefine parameters as optimizable quantities\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weights = torch.tensor(\n    [\n        [1.0, 1.0],\n        [-1.0, 1.0],\n        [-1.0, 1.0],\n        [1.0, 1.0],\n        [-1.0, 1.0],\n        [1.0, 1.0],\n        [1.0, 1.0],\n        [-1.0, 1.0],\n    ],\n    dtype=dtype,\n    device=device,\n)\n\ntorch.autograd.set_detect_anomaly(True)\nsigma = torch.tensor([1.0, 0.5], dtype=dtype, device=device)\na0 = torch.tensor([1.0, 2.0], dtype=dtype, device=device)\n\npositions = positions.detach()\ncell = cell.detach()\npositions.requires_grad_(True)\ncell.requires_grad_(True)\n\nweights = weights.detach()\nsigma = sigma.detach()\na0 = a0.detach()\nweights.requires_grad_(True)\nsigma.requires_grad_(True)\na0.requires_grad_(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the mesh, apply the filter, and also complete the\nPME-like operation by evaluating the transformed mesh\nat the atom positions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "interpolator = torchpme.lib.MeshInterpolator(cell, ns, 3, method=\"Lagrange\")\ninterpolator.compute_weights(positions)\nmesh = interpolator.points_to_mesh(weights)\n\nkernel = ParametricKernel(sigma, a0)\nkernel_filter = torchpme.lib.KSpaceFilter(cell, ns, kernel=kernel)\n\nfiltered = kernel_filter.forward(mesh)\n\nfiltered_at_positions = interpolator.mesh_to_points(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computes a (rather arbitrary) function of the outputs,\nbackpropagates and then outputs the gradients.\nWith this messy non-linear function everything has\nnonzero gradients\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "value = (charges * filtered_at_positions).sum()\nvalue.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    f\"\"\"\nValue: {value}\n\nPosition gradients:\n{positions.grad.T}\n\nCell gradients:\n{cell.grad}\n\nWeights gradients:\n{weights.grad.T}\n\nParam. a0:\n{a0.grad}\n\nParam. sigma:\n{sigma.grad}\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A ``torch`` module based on ``torchpme``\n\nIt is also possible to combine all this in a\ncustom :class:`torch.nn.Module`, which is the\nfirst step towards designing a model training pipeline\nbased on a custom ``torchpme`` model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by defining a Yukawa-like potential, and\na (rather contrieved) model that combines a Fourier\nfilter, with a multi-layer perceptron to post-process\ncharges and \"potential\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the kernel\nclass SmearedCoulomb(torchpme.lib.KSpaceKernel):\n    def __init__(self, sigma2):\n        super().__init__()\n        self._sigma2 = sigma2\n\n    def kernel_from_k_sq(self, k_sq):\n        # we use a mask to set to zero the Gamma-point filter\n        mask = torch.ones_like(k_sq, dtype=torch.bool, device=k_sq.device)\n        mask[..., 0, 0, 0] = False\n        potential = torch.zeros_like(k_sq)\n        potential[mask] = torch.exp(-k_sq[mask] * self._sigma2 * 0.5) / k_sq[mask]\n        return potential\n\n\n# Define the module\nclass KSpaceModule(torch.nn.Module):\n    \"\"\"A demonstrative model combining torchpme and a multi-layer perceptron\"\"\"\n\n    def __init__(\n        self, mesh_spacing: float = 0.5, sigma2: float = 1.0, hidden_sizes=None\n    ):\n        super().__init__()\n        self._mesh_spacing = mesh_spacing\n\n        # degree of smearing as an optimizable parameter\n        self._sigma2 = torch.nn.Parameter(\n            torch.tensor(sigma2, dtype=dtype, device=device)\n        )\n\n        dummy_cell = torch.eye(3, dtype=dtype)\n        self._mesh_interpolator = torchpme.lib.MeshInterpolator(\n            cell=dummy_cell,\n            ns_mesh=torch.tensor([1, 1, 1]),\n            interpolation_nodes=3,\n            method=\"Lagrange\",\n        )\n        self._kernel_filter = torchpme.lib.KSpaceFilter(\n            cell=dummy_cell,\n            ns_mesh=torch.tensor([1, 1, 1]),\n            kernel=SmearedCoulomb(self._sigma2),\n        )\n\n        if hidden_sizes is None:  # default architecture\n            hidden_sizes = [10, 10]\n\n        # a neural network to process \"charge and potential\"\n        last_size = 2  # input is charge and potential\n        self._layers = torch.nn.ModuleList()\n        for hidden_size in hidden_sizes:\n            self._layers.append(\n                torch.nn.Linear(last_size, hidden_size, dtype=dtype, device=device)\n            )\n            self._layers.append(torch.nn.Tanh())\n            last_size = hidden_size\n        self._output_layer = torch.nn.Linear(\n            last_size, 1, dtype=dtype, device=device\n        )  # outputs one value\n\n    def forward(self, positions, cell, charges):\n        # use a helper function to get the mesh size given resolution\n        ns_mesh = torchpme.lib.get_ns_mesh(cell, self._mesh_spacing)\n        ns_mesh = torch.tensor([4, 4, 4])\n\n        self._mesh_interpolator.update(cell=cell, ns_mesh=ns_mesh)\n        self._mesh_interpolator.compute_weights(positions)\n        mesh = self._mesh_interpolator.points_to_mesh(charges)\n\n        self._kernel_filter.update(cell, ns_mesh)\n        mesh = self._kernel_filter.forward(mesh)\n        pot = self._mesh_interpolator.mesh_to_points(mesh)\n\n        x = torch.hstack([charges, pot])\n        for layer in self._layers:\n            x = layer(x)\n        # Output layer\n        x = self._output_layer(x)\n        return x.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates an instance of the model and evaluates it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "my_module = KSpaceModule(sigma2=1.0, mesh_spacing=1.0, hidden_sizes=[10, 4, 10])\n\n# (re-)initialize vectors\n\ncharges = charges.detach()\npositions = positions.detach()\ncell = cell.detach()\ncharges.requires_grad_(True)\npositions.requires_grad_(True)\ncell.requires_grad_(True)\n\nvalue = my_module.forward(positions, cell, charges)\nvalue.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradients compute, and look reasonable!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    f\"\"\"\nValue: {value}\n\nPosition gradients:\n{positions.grad.T}\n\nCell gradients:\n{cell.grad}\n\nCharges gradients:\n{charges.grad.T}\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... also on the MLP parameters!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for layer in my_module._layers:\n    print(layer._parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's always good to run some `gradcheck`...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "my_module.zero_grad()\ncheck = torch.autograd.gradcheck(\n    my_module,\n    (\n        torch.randn((16, 3), device=device, dtype=dtype, requires_grad=True),\n        torch.randn((3, 3), device=device, dtype=dtype, requires_grad=True),\n        torch.randn((16, 1), device=device, dtype=dtype, requires_grad=True),\n    ),\n)\nif check:\n    print(\"gradcheck passed for custom torch-pme module\")\nelse:\n    raise ValueError(\"gradcheck failed for custom torch-pme module\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Jitting a custom module\nThe custom module can also be jitted!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "old_cell_grad = cell.grad.clone()\njit_module = torch.jit.script(my_module)\n\njit_charges = charges.detach()\njit_positions = positions.detach()\njit_cell = cell.detach()\njit_cell.requires_grad_(True)\njit_charges.requires_grad_(True)\njit_positions.requires_grad_(True)\n\njit_value = jit_module.forward(jit_positions, jit_cell, jit_charges)\njit_value.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Values match within machine precision\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    f\"\"\"\nDelta-Value: {value - jit_value}\n\nDelta-Position gradients:\n{positions.grad.T - jit_positions.grad.T}\n\nDelta-Cell gradients:\n{cell.grad - jit_cell.grad}\n\nDelta-Charges gradients:\n{charges.grad.T - jit_charges.grad.T}\n\"\"\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also evaluate the difference in execution\ntime between the Pytorch and scripted versions of the\nmodule (depending on the system, the relative efficiency\nof the two evaluations could go either way, as this is\na too small system to make a difference!)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "duration = 0.0\nfor _i in range(20):\n    my_module.zero_grad()\n    positions = positions.detach()\n    cell = cell.detach()\n    charges = charges.detach()\n    duration -= time()\n    value = my_module.forward(positions, cell, charges)\n    value.backward()\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n    duration += time()\ntime_python = (duration) * 1e3 / 20\n\nduration = 0.0\nfor _i in range(20):\n    jit_module.zero_grad()\n    positions = positions.detach()\n    cell = cell.detach()\n    charges = charges.detach()\n    duration -= time()\n    value = jit_module.forward(positions, cell, charges)\n    value.backward()\n    if device == \"cuda\":\n        torch.cuda.synchronize()\n    duration += time()\ntime_jit = (duration) * 1e3 / 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Evaluation time:\\nPytorch: {time_python}ms\\nJitted:  {time_jit}ms\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}